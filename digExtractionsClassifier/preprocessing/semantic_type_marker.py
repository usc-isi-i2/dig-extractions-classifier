import digExtractionsClassifier.utility.functions as utility_functions

def format_to_mark_semantic_types(all_tokens, tokens_to_mark, semantic_type, tokenization_strategy):
    """
    Just a hack to transform the regular tokens to the same format as should be generated by the pipeline. Would not be used in pipeline
    """
    tokens_new = []
    single_tokens_to_mark = []
    multi_tokens_to_mark = dict()
    for index, token_to_mark in enumerate(tokens_to_mark):
        token_to_mark_tokenized = utility_functions.tokenize(token_to_mark, tokenization_strategy)
        tokens_to_mark_processed = token_to_mark_tokenized[0]
        if(len(token_to_mark_tokenized) > 1):
            #Multi tokens
            if(token_to_mark_tokenized[0] in multi_tokens_to_mark):
                value_list = multi_tokens_to_mark[token_to_mark_tokenized[0]]
                value_list.append(token_to_mark_tokenized)
            else:
                multi_tokens_to_mark[token_to_mark_tokenized[0]] = [token_to_mark_tokenized]

    for index, token in enumerate(all_tokens):
        token_new = dict()
        token_new['value'] = token
        if(bool(re.match('.*[a-zA-Z0-9]',token):
            token_new['type'] = 'normal'
        else:
            token_new['type'] = 'break'
        token_new[]
        if(token in single_tokens_to_mark):
            #Mark it with semantic type
            semantic_type_object = {"type":semantic_type, "offset":0, "length":1 }
            added_semantic_type = token_new.get('semantic_type')
            if(added_semantic_type):
                added_semantic_type.append(semantic_type_object)
            else:
                token_new['semantic_type'] = [semantic_type_object]
        if(token in multi_tokens_to_mark):
            i = index
            list_of_multi_tokens = multi_tokens_to_mark[token]
            for multi_tokens in list_of_multi_tokens:
                print a

                #while i < len(all_tokens) and (i - index) < len(list_of_multi_tokens):
                #    if(multi_tokens_to_mark[i - index] != all_tokens[i]):
                #        break
                #    else:
                    

            #for i in range(index, len(all_tokens)):
            #        same = False


            


            # for multi_token in multi_tokens_to_mark:
                # multi_token index


            